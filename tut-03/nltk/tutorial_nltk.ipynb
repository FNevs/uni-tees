{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d332bc8",
   "metadata": {},
   "source": [
    "# Tutorial NLTK - Jupyter Notebook\n",
    "### Introdução ao Processamento de Linguagem Natural com Python e NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21da2ef",
   "metadata": {},
   "source": [
    "## 1. Conceitos Básicos de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ac8fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis\n",
    "temp = 1\n",
    "nome1 = \"Carla\"\n",
    "frase1 = \"O rato roeu a roupa do rei de Roma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6b3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmorÓdio\n",
      "AmorAmor\n",
      "m\n",
      "ind\n",
      "False\n",
      "iaor\n"
     ]
    }
   ],
   "source": [
    "# Strings\n",
    "letraA = \"a\"\n",
    "letraB = 'b'\n",
    "\n",
    "# Operações com strings\n",
    "var1 = \"Amor\"\n",
    "var2 = \"Ódio\"\n",
    "var3 = \"Ainda que eu falasse a língua dos anjos\"\n",
    "\n",
    "print(var1 + var2)         # Concatenação\n",
    "print(var1 * 2)            # Repetição\n",
    "print(var1[1])             # Indexação\n",
    "print(var3[1:4])           # Fatiamento\n",
    "print('a' in var1)         # Verificação de caractere\n",
    "print(var1.replace('Am', 'ia'))  # Substituição\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91637079",
   "metadata": {},
   "source": [
    "## 2. Expressões Regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db17655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bibliotecário', 'biblioteca']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "texto = \"o bibliotecário abriu a biblioteca\"\n",
    "resultado = re.findall(r'biblio\\w*', texto) # re.findall busca todas as ocorrências de (biblio\\w*) no texto e biblio\\w* significa que a palavra deve começar com \"biblio\" e pode ter qualquer número de caracteres alfanuméricos após isso.\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b196ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pessoa\n"
     ]
    }
   ],
   "source": [
    "texto = \"uma pessoa boa\"\n",
    "resultado = re.search(r'pessoa', texto) # re.search busca a primeira ocorrência de \"pessoa\" no texto\n",
    "print(resultado.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c3684",
   "metadata": {},
   "source": [
    "## 3. Listas e Tokenização com `re`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6bcf4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comida\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de lista representando palavras já tokenizadas\n",
    "texto = ['Casa', ',', 'comida', 'e', 'roupa', 'lavada', '.']\n",
    "print(texto[2])\n",
    "print(len(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659db493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viva pois a história\n",
      "Viva;pois;a;história\n"
     ]
    }
   ],
   "source": [
    "# Reunindo a lista em uma string com espaços ou ponto e vírgula\n",
    "bras = ['Viva', 'pois', 'a', 'história']\n",
    "print(' '.join(bras))\n",
    "print(';'.join(bras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f93a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algum', 'tempo', 'hesitei', 'se', 'devia', 'abrir', 'estas', 'memórias.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenização simples usando expressão regular para dividir por espaço, tabulação ou nova linha\n",
    "texto = \"Algum tempo hesitei se devia abrir estas memórias.\"\n",
    "tokens = re.split(r'[ \\t\\n]+', texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e1a70",
   "metadata": {},
   "source": [
    "## 4. NLTK: Instalação e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc51ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package machado to\n",
      "[nltk_data]     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package machado is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "nltk.download('popular')\n",
    "nltk.download('machado')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc988e31",
   "metadata": {},
   "source": [
    "## 5. Usando Corpus com NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b359d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contos/macn001.txt', 'contos/macn002.txt', 'contos/macn003.txt', 'contos/macn004.txt', 'contos/macn005.txt', 'contos/macn006.txt', 'contos/macn007.txt', 'contos/macn008.txt', 'contos/macn009.txt', 'contos/macn010.txt', 'contos/macn011.txt', 'contos/macn012.txt', 'contos/macn013.txt', 'contos/macn014.txt', 'contos/macn015.txt', 'contos/macn016.txt', 'contos/macn017.txt', 'contos/macn018.txt', 'contos/macn019.txt', 'contos/macn020.txt', 'contos/macn021.txt', 'contos/macn022.txt', 'contos/macn023.txt', 'contos/macn024.txt', 'contos/macn025.txt', 'contos/macn026.txt', 'contos/macn027.txt', 'contos/macn028.txt', 'contos/macn029.txt', 'contos/macn030.txt', 'contos/macn031.txt', 'contos/macn032.txt', 'contos/macn033.txt', 'contos/macn034.txt', 'contos/macn035.txt', 'contos/macn036.txt', 'contos/macn037.txt', 'contos/macn038.txt', 'contos/macn039.txt', 'contos/macn040.txt', 'contos/macn041.txt', 'contos/macn042.txt', 'contos/macn043.txt', 'contos/macn044.txt', 'contos/macn045.txt', 'contos/macn046.txt', 'contos/macn047.txt', 'contos/macn048.txt', 'contos/macn049.txt', 'contos/macn050.txt', 'contos/macn051.txt', 'contos/macn052.txt', 'contos/macn053.txt', 'contos/macn054.txt', 'contos/macn055.txt', 'contos/macn056.txt', 'contos/macn057.txt', 'contos/macn058.txt', 'contos/macn059.txt', 'contos/macn060.txt', 'contos/macn061.txt', 'contos/macn062.txt', 'contos/macn063.txt', 'contos/macn064.txt', 'contos/macn065.txt', 'contos/macn066.txt', 'contos/macn067.txt', 'contos/macn068.txt', 'contos/macn069.txt', 'contos/macn070.txt', 'contos/macn071.txt', 'contos/macn072.txt', 'contos/macn073.txt', 'contos/macn074.txt', 'contos/macn075.txt', 'contos/macn076.txt', 'contos/macn077.txt', 'contos/macn078.txt', 'contos/macn079.txt', 'contos/macn080.txt', 'contos/macn081.txt', 'contos/macn082.txt', 'contos/macn083.txt', 'contos/macn084.txt', 'contos/macn085.txt', 'contos/macn086.txt', 'contos/macn087.txt', 'contos/macn088.txt', 'contos/macn089.txt', 'contos/macn090.txt', 'contos/macn091.txt', 'contos/macn092.txt', 'contos/macn093.txt', 'contos/macn094.txt', 'contos/macn095.txt', 'contos/macn096.txt', 'contos/macn097.txt', 'contos/macn098.txt', 'contos/macn099.txt', 'contos/macn100.txt', 'contos/macn101.txt', 'contos/macn102.txt', 'contos/macn103.txt', 'contos/macn104.txt', 'contos/macn105.txt', 'contos/macn106.txt', 'contos/macn107.txt', 'contos/macn108.txt', 'contos/macn109.txt', 'contos/macn110.txt', 'contos/macn111.txt', 'contos/macn112.txt', 'contos/macn113.txt', 'contos/macn114.txt', 'contos/macn115.txt', 'contos/macn116.txt', 'contos/macn117.txt', 'contos/macn118.txt', 'contos/macn119.txt', 'contos/macn120.txt', 'contos/macn121.txt', 'contos/macn122.txt', 'contos/macn123.txt', 'contos/macn124.txt', 'contos/macn125.txt', 'contos/macn126.txt', 'contos/macn127.txt', 'contos/macn128.txt', 'contos/macn129.txt', 'contos/macn130.txt', 'contos/macn131.txt', 'contos/macn132.txt', 'contos/macn133.txt', 'contos/macn134.txt', 'contos/macn135.txt', 'contos/macn136.txt', 'contos/macn137.txt', 'critica/mact01.txt', 'critica/mact02.txt', 'critica/mact03.txt', 'critica/mact04.txt', 'critica/mact05.txt', 'critica/mact06.txt', 'critica/mact07.txt', 'critica/mact08.txt', 'critica/mact09.txt', 'critica/mact10.txt', 'critica/mact11.txt', 'critica/mact12.txt', 'critica/mact13.txt', 'critica/mact14.txt', 'critica/mact15.txt', 'critica/mact16.txt', 'critica/mact17.txt', 'critica/mact18.txt', 'critica/mact19.txt', 'critica/mact20.txt', 'critica/mact21.txt', 'critica/mact22.txt', 'critica/mact23.txt', 'critica/mact24.txt', 'critica/mact25.txt', 'critica/mact26.txt', 'critica/mact27.txt', 'critica/mact28.txt', 'critica/mact29.txt', 'critica/mact30.txt', 'critica/mact31.txt', 'critica/mact32.txt', 'critica/mact33.txt', 'critica/mact34.txt', 'critica/mact35.txt', 'critica/mact36.txt', 'critica/mact37.txt', 'critica/mact38.txt', 'critica/mact39.txt', 'critica/mact40.txt', 'critica/mact41.txt', 'critica/mact42.txt', 'critica/mact43.txt', 'critica/mact44.txt', 'critica/mact45.txt', 'cronica/macr01.txt', 'cronica/macr02.txt', 'cronica/macr03.txt', 'cronica/macr04.txt', 'cronica/macr05.txt', 'cronica/macr06.txt', 'cronica/macr07.txt', 'cronica/macr08.txt', 'cronica/macr09.txt', 'cronica/macr10.txt', 'cronica/macr11.txt', 'cronica/macr12.txt', 'cronica/macr13.txt', 'cronica/macr14.txt', 'cronica/macr15.txt', 'cronica/macr16.txt', 'cronica/macr17.txt', 'cronica/macr18.txt', 'cronica/macr19.txt', 'cronica/macr20.txt', 'cronica/macr21.txt', 'cronica/macr22.txt', 'cronica/macr23.txt', 'cronica/macr24.txt', 'miscelanea/mams01.txt', 'miscelanea/mams02.txt', 'miscelanea/mams03.txt', 'miscelanea/mams04.txt', 'miscelanea/mams05.txt', 'miscelanea/mams06.txt', 'miscelanea/mams07.txt', 'miscelanea/mams08.txt', 'miscelanea/mams09.txt', 'miscelanea/mams10.txt', 'poesia/maps01.txt', 'poesia/maps02.txt', 'poesia/maps03.txt', 'poesia/maps04.txt', 'poesia/maps05.txt', 'poesia/maps06.txt', 'poesia/maps07.txt', 'romance/marm01.txt', 'romance/marm02.txt', 'romance/marm03.txt', 'romance/marm04.txt', 'romance/marm05.txt', 'romance/marm06.txt', 'romance/marm07.txt', 'romance/marm08.txt', 'romance/marm09.txt', 'romance/marm10.txt', 'teatro/matt01.txt', 'teatro/matt02.txt', 'teatro/matt03.txt', 'teatro/matt04.txt', 'teatro/matt05.txt', 'teatro/matt06.txt', 'teatro/matt07.txt', 'teatro/matt08.txt', 'teatro/matt09.txt', 'teatro/matt10.txt', 'traducao/matr01.txt', 'traducao/matr02.txt', 'traducao/matr03.txt']\n",
      "essa anônima, ainda que não parenta, padeceu mais do que as parentas.\n",
      "É verdade, padeceu mais. Não digo que se carpisse, não digo que se deixasse\n",
      "rolar pelo chão, convulsa. Nem o meu óbito era coisa a\n",
      "77098\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import machado\n",
    "\n",
    "# IDs dos arquivos disponíveis\n",
    "print(machado.fileids())\n",
    "\n",
    "# Texto cru e tokens\n",
    "raw_text = machado.raw('romance/marm05.txt')\n",
    "print(raw_text[5600:5800])\n",
    "\n",
    "texto1 = machado.words('romance/marm05.txt')\n",
    "print(len(texto1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e085162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 138 matches:\n",
      "De pé , à cabeceira da cama , com os olhos estúpidos , a boca entreaberta , a t\n",
      "orelhas . Pela minha parte fechei os olhos e deixei - me ir à ventura . Já agor\n",
      "xões de cérebro enfermo . Como ia de olhos fechados , não via o caminho ; lembr\n",
      "gelos eternos . Com efeito , abri os olhos e vi que o meu animal galopava numa \n",
      "me apareceu então , fitando - me uns olhos rutilantes como o sol . Tudo nessa f\n",
      " mim mesmo . Então , encarei - a com olhos súplices , e pedi mais alguns anos .\n",
      "o alto de uma montanha . Inclinei os olhos a uma das vertentes , e contemplei ,\n",
      "ilhão , e , não obstante , porque os olhos do delírio são outros , eu via tudo \n",
      "cifração da eternidade . E fixei os olhos , e continuei a ver as idades , que \n",
      " esperto , concordava meu pai ; e os olhos babavam - se - lhe de orgulho , e el\n",
      "te , e , repetido o mote , cravar os olhos na testa de uma senhora , depois tos\n",
      "avrear de estômagos satisfeitos ; os olhos moles e úmidos , ou vivos e cálidos \n",
      "m estacado de orquestra , e todos os olhos se voltavam para o glosador . Quem f\n",
      " . Eu via isso , porque arrastava os olhos da compota para ele e dele para a co\n",
      " eu segui - os . O Vilaça levava nos olhos umas chispas de vinho e de volúpia .\n",
      "es ... D . Eusébia levou o lenço aos olhos . O glosador vasculhava na memória a\n",
      " estupefação imobilizou a todos ; os olhos espraiavam - se a uma e outra banda \n",
      "a aula , dava um pulo , circulava os olhos chamejantes , dizia - nos os últimos\n",
      ", deixava - se estar quieto , com os olhos espetados no ar . Uma flor , o Quinc\n",
      "u forcejava por trazer a bigode . Os olhos , vivos e resolutos , eram a minha f\n",
      " pensativa , ou levantou para mim os olhos cobiçosos . De todas porém a que me \n",
      "pouco ou nada comi , porque só tinha olhos para a dona da casa . Que gentil que\n",
      "erramava - se - lhe a felicidade dos olhos , e eu sentia - me feliz com vê - la\n",
      " meu amor ! Eu agradeci - lho com os olhos úmidos . No dia seguinte levei - lhe\n",
      "proposta . Marcela ouviu - me com os olhos no ar , sem responder logo ; como in\n",
      "já leitor lhe dizer com menos contar deixei cheguei desferirem peito\n",
      "corpo fazer porém ver casamento soltar quebrei titubear sacrifício\n",
      "estúpidos; e; fechados; e; rutilantes; súplices; a; do; ,; babavam;\n",
      "na; moles; se; da; umas; .; espraiavam; chamejantes; espetados; ,;\n",
      "cobiçosos; para; ,; úmidos; no; ;; de; de; fitos; a; naquele; do; ,;\n",
      "pretos; as; estúpidos; ao; às; ...; ,; fúlgidos; de; ,; .; de; pretos;\n",
      "tão; de; para; a; chisparam; para; me; da; ,; ,; uma; no; na; para;\n",
      "se; em; .; em; .; de; ,; no; nela; tinham; ;; cintilantes; o; dos; e;\n",
      ",; de; de; dela; vermelhos; .; e; .; o; ,; constantemente; para; ,; ,;\n",
      "para; ,; ao; ,; na; na; baixos; no; mais; no; se; dela; do; no; ,;\n",
      "lampejantes; rasos; todos; ,; e; do; pelos; de; ao; .; lhe; de;\n",
      "enfermos; :; ,; .; e; da; fixos; .; fitos; ,; ,; bonitos; de; ...; .;\n",
      "de; algum; a; ;; fitos; em\n"
     ]
    }
   ],
   "source": [
    "# Criando um objeto Text do NLTK para explorar funcionalidades como concordância e colocação\n",
    "from nltk.text import Text\n",
    "bras = Text(texto1)\n",
    "\n",
    "# Concordância\n",
    "bras.concordance('olhos')\n",
    "\n",
    "# Palavras similares\n",
    "bras.similar('chegar')\n",
    "\n",
    "# Padrão com expressão regular\n",
    "bras.findall(\"<olhos> (<.*>)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cce2aa",
   "metadata": {},
   "source": [
    "## 6. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daefdb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n"
     ]
    }
   ],
   "source": [
    "# Carrega uma lista de stopwords (palavras irrelevantes) para o português\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644fd23",
   "metadata": {},
   "source": [
    "## 7. Colocações e Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3a11e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('meu', 'coração'), ('coração', 'está'), ('está', 'bem'), ('bem', 'machucado')]\n"
     ]
    }
   ],
   "source": [
    "# Gera pares de palavras (bigramas) a partir de uma lista \n",
    "# Bigramas são pares de palavras consecutivas em um texto\n",
    "from nltk import bigrams\n",
    "tokens = ['meu', 'coração', 'está', 'bem', 'machucado']\n",
    "print(list(bigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91292ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quincas Borba; Lobo Neves; alguma coisa; Brás Cubas; meu pai; que não;\n",
      "dia seguinte; não sei; Com efeito; que era; Meu pai; alguns instantes;\n",
      "outra vez; outra coisa; por exemplo; que ele; mim mesmo; coisa\n",
      "nenhuma; mesma coisa; não era\n"
     ]
    }
   ],
   "source": [
    "# Mostra combinações de palavras que aparecem frequentemente juntas\n",
    "bras.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081f31c",
   "metadata": {},
   "source": [
    "## 8. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9974045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copi\n",
      "pais\n"
     ]
    }
   ],
   "source": [
    "# Aplica o stemmer RSLP para reduzir palavras à sua raiz\n",
    "# Stemmer é uma técnica de redução de palavras que remove sufixos para encontrar a raiz da palavra\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "print(stemmer.stem(\"copiar\"))\n",
    "print(stemmer.stem(\"paisagem\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4abf0",
   "metadata": {},
   "source": [
    "## 9. Tokenização com NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "615825ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estou', 'bem', ',', 'mas', 'não', 'tenho', 'certeza', 'se', 'viajarei', 'amanhã', 'às', '8:30', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokeniza a string em palavras e pontuação com base em regras linguísticas\n",
    "sentence = \"Estou bem, mas não tenho certeza se viajarei amanhã às 8:30.\"\n",
    "tokens = nltk.word_tokenize(sentence) #word_tokenize é uma função do NLTK que divide a string em tokens, considerando palavras e pontuação.\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501972c",
   "metadata": {},
   "source": [
    "## 10. POS Tagging com MacMorpho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aeae6bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n",
      "[('Estou', 'N'), ('bem', 'N'), (',', 'N'), ('mas', 'N'), ('não', 'N'), ('tenho', 'N'), ('certeza', 'N'), ('se', 'N'), ('viajarei', 'N'), ('amanhã', 'N'), ('às', 'N'), ('8:30', 'N'), ('.', 'N')]\n"
     ]
    }
   ],
   "source": [
    "# Carrega sentenças etiquetadas do corpus MacMorpho para treinamento ou testes\n",
    "from nltk.corpus import mac_morpho\n",
    "\n",
    "sentencas_etiquetadas = mac_morpho.tagged_sents() # tagged_sents retorna uma lista de sentenças, onde cada sentença é uma lista de tuplas (palavra, etiqueta).\n",
    "tags = [tag for (word, tag) in mac_morpho.tagged_words()] # tagged_words retorna uma lista de tuplas (palavra, etiqueta) para todas as palavras do corpus.\n",
    "mais_frequente = nltk.FreqDist(tags).max() # Obtém a etiqueta mais frequente no corpus.\n",
    "print(mais_frequente)\n",
    "\n",
    "# Tagger padrão\n",
    "etiqPadrao = nltk.tag.DefaultTagger('N') # Cria um tagger que atribui a etiqueta 'N' (substantivo) a todas as palavras.\n",
    "print(etiqPadrao.tag(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408467f",
   "metadata": {},
   "source": [
    "## 11. UnigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd5e37a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Estou', 'V'), ('bem', 'ADV'), (',', ','), ('mas', 'KC'), ('não', 'ADV'), ('tenho', 'V'), ('certeza', 'N'), ('se', 'PROPESS'), ('viajarei', None), ('amanhã', 'ADV'), ('às', 'ADV'), ('8:30', None), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Cria um etiquetador baseado na palavra individual mais provável (unigrama)\n",
    "unigram_tagger = nltk.tag.UnigramTagger(sentencas_etiquetadas) # UnigramTagger é um etiquetador que usa a probabilidade de uma palavra individual para atribuir etiquetas.\n",
    "print(unigram_tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d14ef9",
   "metadata": {},
   "source": [
    "## 12. Bigram e Trigram Tagger com Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ee9d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um etiquetador padrão que marca todas as palavras como substantivo (N)\n",
    "t0 = nltk.DefaultTagger('N')\n",
    "t1 = nltk.UnigramTagger(sentencas_etiquetadas, backoff=t0)\n",
    "t2 = nltk.BigramTagger(sentencas_etiquetadas, backoff=t1)\n",
    "t3 = nltk.TrigramTagger(sentencas_etiquetadas, backoff=t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c295f6",
   "metadata": {},
   "source": [
    "## 13. Salvando e Carregando o Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f3632b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o etiquetador treinado em um arquivo usando pickle\n",
    "# pickle é um módulo do Python que permite serializar e desserializar objetos, facilitando o armazenamento e a recuperação de objetos complexos.\n",
    "from pickle import dump, load\n",
    "\n",
    "# Salvar\n",
    "with open('mac_morpho.pkl', 'wb') as output:\n",
    "    dump(t3, output, -1)\n",
    "\n",
    "# Carregar\n",
    "with open('mac_morpho.pkl', 'rb') as input_file:\n",
    "    tagger = load(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c226b39",
   "metadata": {},
   "source": [
    "## 14. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5e25f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S João/NPROP comprou/V um/ART (PADRAO carro/N esportivo/ADJ))\n"
     ]
    }
   ],
   "source": [
    "# Tokeniza a string em palavras e pontuação com base em regras linguísticas\n",
    "text = \"João comprou um carro esportivo\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = tagger.tag(tokens)\n",
    "gramatica = r\"\"\"PADRAO: {<N><ADJ>+}\"\"\"\n",
    "analiseGramatical = nltk.RegexpParser(gramatica)\n",
    "tree = analiseGramatical.parse(tagged)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f91c0f",
   "metadata": {},
   "source": [
    "## 15. Named Entity Recognition (NER) com Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b7f7892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  O/ART\n",
      "  ministro/N\n",
      "  (NE Edson/NPROP)\n",
      "  Fachin/N\n",
      "  do/KS\n",
      "  (NE Supremo/NPROP Tribunal/NPROP Federal/NPROP)\n",
      "  determinou/V\n",
      "  a/ART\n",
      "  separação/N\n",
      "  do/KS\n",
      "  inquérito/N\n",
      "  contra/PREP\n",
      "  o/ART\n",
      "  presidente/N\n",
      "  (NE Michel/NPROP Temer/NPROP da/NPROP)\n",
      "  investigação/N\n",
      "  contra/PREP\n",
      "  o/ART\n",
      "  senador/N\n",
      "  afastado/PCP\n",
      "  (NE Aécio/NPROP Neves/NPROP ./NPROP))\n"
     ]
    }
   ],
   "source": [
    "# Tokeniza a string em palavras e pontuação com base em regras linguísticas\n",
    "text = \"O ministro Edson Fachin do Supremo Tribunal Federal determinou a separação do inquérito contra o presidente Michel Temer da investigação contra o senador afastado Aécio Neves.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = tagger.tag(tokens)\n",
    "gramatica = r\"\"\"NE: {<NPROP>+}\"\"\"\n",
    "parser = nltk.RegexpParser(gramatica)\n",
    "ner_tree = parser.parse(tagged)\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74824efb",
   "metadata": {},
   "source": [
    "## 16. Extração de Relações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d89fa783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  A/ART\n",
      "  estátua/N\n",
      "  foi/VAUX\n",
      "  criada/PCP\n",
      "  pelo/PDEN\n",
      "  escultor/N\n",
      "  (REL\n",
      "    (NE George/NPROP Silva/NPROP)\n",
      "    ,/,\n",
      "    que/PRO-KS-REL\n",
      "    nasceu/V\n",
      "    na/PREP|+\n",
      "    (NE Romênia/NPROP)))\n"
     ]
    }
   ],
   "source": [
    "# Tokeniza a string em palavras e pontuação com base em regras linguísticas\n",
    "text = 'A estátua foi criada pelo escultor George Silva, que nasceu na Romênia'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = tagger.tag(tokens)\n",
    "gramatica = r\"\"\" \n",
    "    NE: {<NPROP>+}\n",
    "    REL: {<NE> <.*>* <PREP.*> <.*>* <NE>} \"\"\"\n",
    "parser = nltk.RegexpParser(gramatica)\n",
    "rel_tree = parser.parse(tagged)\n",
    "print(rel_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
